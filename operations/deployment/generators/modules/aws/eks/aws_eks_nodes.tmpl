resource "aws_eks_cluster" "main" {
  name     = var.aws_eks_cluster_name
  role_arn = aws_iam_role.iam_role_master.arn
  vpc_config {
    security_group_ids      = [aws_security_group.eks_security_group_master.id]
    subnet_ids              = module.eks_vpc.public_subnets
    endpoint_private_access = false
    endpoint_public_access  = true
  }

  depends_on = [
    module.eks_vpc,
    module.eks_vpc.public_subnets,
    aws_iam_role.iam_role_master,
    aws_security_group.eks_security_group_master
  ]
  version    = var.aws_eks_cluster_version
  enabled_cluster_log_types = var.aws_eks_cluster_log_types

  tags = {
    "kubernetes.io/cluster/${var.aws_eks_cluster_name}" = "owned"
  }
}

data "aws_eks_cluster" "eks-cluster" {
  name = aws_eks_cluster.main.id
}

data "aws_eks_cluster_auth" "cluster_auth" {
  name = aws_eks_cluster.main.id
}

provider "kubernetes" {
  host                   = aws_eks_cluster.main.endpoint
  cluster_ca_certificate = base64decode(aws_eks_cluster.main.certificate_authority.0.data)
  token                  = data.aws_eks_cluster_auth.main.token
  #load_config_file       = false
}

#########

#resource "aws_launch_configuration" "main" {
#  associate_public_ip_address = true
#  iam_instance_profile        = aws_iam_instance_profile.eks_inst_profile.name
#  image_id                    = var.aws_eks_image_id
#  instance_type               = var.aws_eks_instance_type
#  name_prefix                 = "${var.aws_eks_environment}-eksworker"
#  security_groups             = [aws_security_group.eks_security_group_worker.id] #[aws_security_group.node.id]
#  user_data                   = filebase64(try(file("./aws_eks_incoming_user_data_script.sh"), ""))
#  key_name                    = var.aws_eks_ec2_key_pair
#  lifecycle {
#    create_before_destroy = true
#  }
#}

resource "aws_launch_template" "main" {
  network_interfaces {
    associate_public_ip_address = true
  }
  iam_instance_profile {
    name = aws_iam_instance_profile.eks_inst_profile.name
  }
  image_id                    = var.aws_eks_image_id
  instance_type               = var.aws_eks_instance_type
  name_prefix                 = "${var.aws_eks_environment}-eksworker"
  #security_group_names        = [aws_security_group.eks_security_group_worker.name]
  vpc_security_group_ids      = [aws_security_group.eks_security_group_worker.id]
  user_data                   = base64encode(try(file("./aws_eks_incoming_user_data_script.sh"), ""))
  key_name                    = var.aws_eks_ec2_key_pair
  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_autoscaling_group" "main" {
  desired_capacity     = var.aws_eks_desired_capacity
  #launch_configuration = aws_launch_configuration.main.id
  launch_template {
    id      = aws_launch_template.main.id
  }
  max_size             = var.aws_eks_max_size
  min_size             = var.aws_eks_min_size
  name                 = "${var.aws_eks_environment}-eksworker-asg"
  vpc_zone_identifier  = [for s in module.eks_vpc.private_subnets: s.id]
  #vpc_zone_identifier  = module.eks_vpc.private_subnets
  #vpc_zone_identifier  = module.eks_vpc.id
  health_check_type    = "EC2"


tag {
  key                 = "kubernetes.io/cluster/${var.aws_eks_cluster_name}"
  value               = "owned"
  propagate_at_launch = true
}

/* [tag for tag_key, tag_value in var.common_tags: {
  key                 = tag_key
  value               = tag_value
  propagate_at_launch = true
}] */


  depends_on = [
    module.eks_vpc,
    module.eks_vpc.private_subnets,
    aws_iam_role.iam_role_master,
    aws_iam_role.iam_role_worker,
    aws_security_group.eks_security_group_master,
    aws_security_group.eks_security_group_worker
  ]
}