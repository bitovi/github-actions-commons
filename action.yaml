name: 'Deploy Docker to AWS (EC2)'
description: 'Deploy a Docker app to an AWS Virtual Machine (EC2) with Docker Compose'
branding:
  icon: upload-cloud
  color: red
inputs:
  # GitHub Commons main inputs
  checkout:
    description: 'Specifies if this action should checkout the code'
    required: false
    default: 'true'
  bitops_skip_run:
    description: 'Will run the setup of the BitOps environment and exit. Only usefull to debug startup scripts.'
    required: false
    default: 'false'
  bitops_code_only:
    description: 'Will run only the generation phase of BitOps, where the Terraform and Ansible code is built.'
    required: false
  bitops_code_store:
    description: 'Store BitOps code as a GitHub artifact'
    required: false
  bitops_extra_env_vars:
    description: 'Variables to be passed to BitOps as Docker extra vars. Format should be `-e KEY1=VALUE1 -e KEY2=VALUE2`'
    required: false
  bitops_extra_env_vars_file:
    description: '.env file to pass to BitOps Docker run. Usefull for long variables.'
    required: false
  tf_stack_destroy:
    description: 'Set to "true" to Destroy the stack through Terraform.'
    required: false
  tf_state_bucket:
    description: 'AWS S3 bucket to use for Terraform state. Defaults to `${org}-${repo}-{branch}-tf-state`'
    required: false
  tf_state_bucket_destroy:
    description: 'Force purge and deletion of S3 bucket defined. Any file contained there will be destroyed. `tf_stack_destroy` must also be `true`'
    required: false
  tf_state_bucket_provider: 
    description: 'Bucket provider for tfstate storage.'
    required: false
    default: 'aws'
  tf_targets:
    description: 'A list of targets to create before the full stack creation.'
    required: false
  ansible_skip:
    description: 'Skip Ansible execution after Terraform excecution.'
    required: false

  # GitHub Deployment repo inputs
  gh_deployment_input_terraform:
    description: 'Folder to store Terraform files to be included during Terraform execution.'
    required: false
  gh_deployment_input_ansible:
    description: 'Folder where a whole Ansible structure is expected. If missing bitops.config.yaml a default will be generated.'
    required: false
  gh_deployment_input_ansible_playbook:
    description: 'Main playbook to be looked for.'
    default: playbook.yml
    required: false
  gh_deployment_input_ansible_extra_vars_file:
    description: 'Relative path to file from project root to Ansible vars file to be applied. '
    required: false
  gh_deployment_action_input_ansible_extra_vars_file:
    description: 'Relative path to file from project root to Ansible vars file to be applied into the Action Ansible execution. '
    required: false
  gh_deployment_input_helm_charts: 
    description: 'Relative path to the folder from project containing Helm charts to be installed. Could be uncompressed or compressed (.tgz) files.'
    required: false

  # GitHub Action repo inputs
  gh_action_repo:
    description: 'URL of calling repo'
    required: false
  gh_action_input_terraform:
    description: 'Folder to store Terraform files to be included during Terraform execution.'
    required: false
  gh_action_input_ansible:
    description: 'Folder where a whole Ansible structure is expected. If missing bitops.config.yaml a default will be generated.'
    required: false
  gh_action_input_ansible_playbook:
    description: 'Main playbook to be looked for.'
    default: playbook.yml
    required: false
  gh_action_input_helm_charts:
    description: 'Relative path to the folder from action containing Helm charts to be installed. Could be uncompressed or compressed (.tgz) files.'
    required: false

  # AWS Specific
  aws_access_key_id:
    description: 'AWS access key ID'
    required: false
  aws_secret_access_key:
    description: 'AWS secret access key'
    required: false
  aws_session_token:
    description: 'AWS session token'
    required: false
  aws_default_region:
    description: 'AWS default region'
    default: us-east-1
    required: false
  aws_resource_identifier:
    description: 'Set to override the AWS resource identifier for the deployment.  Defaults to `${org}-{repo}-{branch}`.  Use with destroy to destroy specific resources.'
    required: false
  aws_additional_tags:
    description: 'A JSON object of additional tags that will be included on created resources. Example: `{"key1": "value1", "key2": "value2"}`'
    required: false

  # ENV files
  env_aws_secret:
    description: 'Secret name to pull env variables from AWS Secret Manager'
    required: false
  env_repo:
    description: 'File containing environment variables to be used with the app'
    required: false
  env_ghs:
    description: '`.env` file to be used with the app from Github secrets'
    required: false
  env_ghv:
    description: '`.env` file to be used with the app from Github variables'
    required: false

  # EC2 Instance
  aws_ec2_instance_create:
    description: 'Define if an EC2 instance should be created'
    required: false
  aws_ec2_ami_filter:
    description: 'AWS AMI Filter string. Will be used to lookup for lates image based on the string. Defaults to `ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*`.'
    required: false
  aws_ec2_ami_owner:
    description: 'Owner of AWS AMI image. This ensures the provider is the one we are looking for. Defaults to `099720109477`, Canonical (Ubuntu).'
    required: false
  aws_ec2_ami_id:
    description: 'AWS AMI ID. Will default to lookup for latest image of the `aws_ec2_ami_filter` string. This will override `aws_ec2_ami_filter` lookup.'
    required: false
  aws_ec2_ami_update:
    description: 'Set this to true if you want to recreate the EC2 instance if there is a newer version of the AMI.'
    required: false
  aws_ec2_iam_instance_profile:
    description: 'The AWS IAM instance profile to use for the EC2 instance'
    required: false
  aws_ec2_instance_type: 
    description: 'The AWS Instance type'
    required: false
  aws_ec2_instance_root_vol_size:
    description: 'Define the volume size (in GiB) for the root volume on the AWS Instance.'
    required: false
  aws_ec2_instance_root_vol_preserve:
    description: 'Set this to true to avoid deletion of root volume on termination. Defaults to false.'
    required: false
  aws_ec2_security_group_name:
    description: 'The name of the EC2 security group'
    required: false
  aws_ec2_create_keypair_sm:
    required: false
    description: 'Generates and manages a secret manager entry that contains the public and private keys created for the ec2 instance.'
  aws_ec2_instance_public_ip:
    description: 'Add a public IP to the instance or not. (Not an Elastic IP)'
    required: false
  aws_ec2_port_list:
    description: 'List of ports to be enabled as an ingress rule in the EC2 SG, in a [xx,yy] format - Not the ELB'
    required: false
  aws_ec2_user_data_file:
    description: 'Relative path in the repo for a user provided script to be executed with Terraform EC2 Instance creation.'
    required: false
  aws_ec2_user_data_replace_on_change:
    description: 'If user_data file changes, instance will stop and start. Hence public IP will change. Defaults to true.'
    required: false
  
  # AWS Route53 Domains abd Certificates
  aws_r53_enable:
    description: 'Enables the usage of Route53 to manage DNS records.'
    required: false
  aws_r53_domain_name:
    description: 'Define the root domain name for the application. e.g. app.com'
    required: false
  aws_r53_sub_domain_name:
    description: 'Define the sub-domain part of the URL. Defaults to `${org}-${repo}-{branch}`'
  aws_r53_root_domain_deploy:
    description: 'Deploy to root domain. Will generate two DNS recrods, one for root, another for www'
    required: false
  aws_r53_enable_cert:
    description: 'Makes the application use a certificate by enabling a certificate lookup.'
    required: false
  aws_r53_cert_arn:
    description: 'Define the certificate ARN to use for the application'
    required: false
  aws_r53_create_root_cert:
    description: 'Generates and manage the root cert for the application'
    required: false
  aws_r53_create_sub_cert: 
    description: 'Generates and manage the sub-domain certificate for the application'
    required: false

  # AWS ELB
  aws_elb_create:
    description: 'Create a load balancer and map ports to the EC2 instance.'
    required: false
  aws_elb_security_group_name:
    description:  'The name of the ELB security group'
    required: false
  aws_elb_app_port:
    description: 'Port to expose for the app'
    required: false
  aws_elb_app_protocol:
    description: 'Protocol to enable. Could be HTTP, HTTPS, TCP or SSL. Defaults to TCP.'
    required: false
  aws_elb_listen_port:
    description: 'Load balancer listening port. Defaults to 80 if NO FQDN provided, 443 if FQDN provided'
    required: false
  aws_elb_listen_protocol:
    description: 'Protocol to enable. Could be HTTP, HTTPS, TCP or SSL. Defaults to TCP if NO FQDN provided, SSL if FQDN provided'
    required: false
  aws_elb_healthcheck: 
    description: 'Load balancer health check string. Defaults to TCP:22'
    required: false
  
  # AWS EFS
  aws_efs_create:
    description: 'Toggle to indicate whether to create and EFS and mount it to the ec2 as a part of the provisioning. Note: The EFS will be managed by the stack and will be destroyed along with the stack'
    required: false
  aws_efs_create_ha:
    description: 'Toggle to indicate whether the EFS resource should be highly available (target mounts in all available zones within region)'
    required: false
  aws_efs_mount_id:
    description: 'ID of existing EFS'
    required: false
  aws_efs_mount_security_group_id:
    description: 'ID of the primary security group used by the existing EFS'
    required: false
  aws_efs_security_group_name:
    description:  'The name of the EFS security group'
    required: false
  aws_efs_create_replica:
    description: 'Toggle to indiciate whether a read-only replica should be created for the EFS primary file system'
    required: false
  aws_efs_enable_backup_policy:
    description: 'Toggle to indiciate whether the EFS should have a backup policy, default is false'
    required: false
  aws_efs_zone_mapping:
    description: 'Information on Zone Mapping can be found in the [README.md](README.md#efs-zone-mapping)'
    required: false
  aws_efs_transition_to_inactive:
    description: 'Indicates how long it takes to transition files to the IA storage class.'
    required: false
  aws_efs_replication_destination:
    description: 'AWS Region to target for replication'
    required: false
  aws_efs_mount_target: 
    description: 'Directory path in the EFS volume to mount directory to. Default is /.'
    required: false
  aws_efs_ec2_mount_point:
    description: 'Directory path in EC2 Instance to mount the EFS volume. Default is `data`. Exported as `HOST_DIR` in `.env`'
    required: false
  
  # AWS RDS
  aws_aurora_enable:
    description: 'Set to "true" to enable a postgres database'
    required: false
  aws_aurora_engine: 
    description: 'Which Database engine to use'
    required: false
  aws_aurora_engine_version:
    description: 'Specify Postgres version'
    required: false
  aws_aurora_database_group_family:
    description: 'Postgres database group family'
    required: false 
  aws_aurora_instance_class:
    description: 'Define the size of the instances in the DB cluster'
    required: false
  aws_aurora_security_group_name:
    description:  'The name of the Postgres security group'
    required: false
  aws_aurora_subnets:
    description: 'Specify which subnets to use as a list of strings.  Example: `i-1234,i-5678,i-9101`'
    required: false
  aws_aurora_cluster_name:
    description: 'Specify a cluster name. Will be created if it does not exist'
    required: false
  aws_aurora_database_name:
    description: 'Specify a database name. Will be created if it does not exist'
    required: false
  aws_aurora_database_port:
    description: 'Postgres database port'
    required: false
  aws_aurora_restore_snapshot:
    description: 'Restore a snapshot to the DB. Should be used only once. Changes in this value will destroy and recreate the database completely.'
    required: false
  aws_aurora_snapshot_name:
    description: 'Takes a snapshot of the cluster using that name. If none definded, no snapshot will be made. If snap already exists, no new one will be created.'
    required: false
  aws_aurora_snapshot_overwrite:
    description: 'If the snapshot name is the same as an existing one, will destroy and create a new one.'
    required: false
  aws_aurora_database_protection:
    description: 'Protects the database from deletion.'
    required: false
  aws_aurora_database_final_snapshot:
    description: 'Generates a snapshot of the database before deletion.'
    required: false
    
  # Docker 
  docker_install: 
    description: 'Define if docker should be installed. After this, docker-compose up will be excecuted.'
    required: false
  docker_remove_orphans:
    description: 'Toggle --remove-orphans flag. Defaults to false.'
    required: false
  docker_full_cleanup: 
    description: 'Set to true to run docker-compose down and docker system prune --all --force --volumes after.'
    required: false
  docker_repo_app_directory:
    description: 'Relative path for the directory of the app (i.e. where `Dockerfile` and `docker-compose.yaml` files are located). This is the directory that is copied to the compute instance (EC2).  Default is the root of the repo. Add a .gha-ignore file with a list of files to be exluded. '
    required: false
  docker_repo_app_directory_cleanup:
    description: 'Will generate a timestamped compressed file and delete the app repo directory.'
    required: false
  docker_efs_mount_target:
    description: 'Directory path within docker env to mount directory to, default is `/data`'
    required: false

  # AWS EKS
  aws_eks_create:
    description: 'Define if an EKS cluster should be created'
    required: false
  aws_eks_region:
    description: 'Define the region where EKS cluster should be created'
    required: false
  aws_eks_security_group_name_master:
    description:  "SG for ${var.aws_resource_identifier} - ${var.aws_eks_environment} - EKS Master"
    required: false
  aws_eks_security_group_name_worker:
    description:  "SG for ${var.aws_resource_identifier} - ${var.aws_eks_environment} - EKS Worker"
    required: false
  aws_eks_environment:
    description: 'Specify the eks environment name. Defaults to env'
    required: false
  aws_eks_stackname:
    description: 'Specify the eks stack name for your environment. Defaults to eks-stack'
    required: false  
  aws_eks_cidr_block:
    description: 'Define Base CIDR block which is divided into subnet CIDR blocks (e.g. `10.0.0.0/16`)'
    required: false  
  aws_eks_workstation_cidr:
    description: 'Enter your local workstation public IP to add it to Worker nodes security groups' 
    required: false 
  aws_eks_availability_zones:
    description: "List of Availability Zones, Ex: `['us-east-1a', 'us-east-1b', 'us-east-1c']`"
    required: false 
  aws_eks_private_subnets:
    description:  "List of private subnets (e.g. `['10.0.1.0/24', '10.0.2.0/24']`)"
    required: false 
  aws_eks_public_subnets:
    description:  "List of public subnets (e.g. `['10.0.101.0/24', '10.0.102.0/24']`)"
    required: false 
  aws_eks_cluster_name:
    description: "EKS Cluster name. Defaults to eks-cluster"
    required: false
  aws_eks_cluster_log_types:
    description: "EKS Log types, csv list"
    required: false
  aws_eks_cluster_version:
    description: 'Specify the k8s cluster version'
    required: false 
  aws_eks_instance_type:
    description: 'enter the aws instance type'
    required: false 
  aws_eks_instance_ami_id:
    description: 'AWS AMI ID. Will default to the latest Amazon EKS Node image for the cluster version.'
    required: false
  aws_eks_instance_user_data_file:
    description: 'Relative path in the repo for a user provided script to be executed with Terraform EKS Node creation.'
    required: false
  aws_eks_ec2_key_pair:
    description: 'Enter the existing ec2 key pair for worker nodes. If none, one will be created.'
    required: false 
  aws_eks_store_keypair_sm:
    description: 'If true, will store the newly created keys in Secret Manager'
    required: false 
  aws_eks_desired_capacity:
    description: 'Enter the desired capacity for the worker nodes' 
    required: false
  aws_eks_max_size:
    description: 'Enter the max_size for the worker nodes' 
    required: false
  aws_eks_min_size:
    description: 'Enter the min_size for the worker nodes'
    required: false

outputs:
  vm_url:
    description: "The URL of the generated app"
    value: ${{ steps.deploy.outputs.vm_url }}

runs:
  using: 'composite'
  steps:
    - name: Checkout if required
      if: ${{ inputs.checkout == 'true' }}
      uses: actions/checkout@v3

    - name: Deploy with BitOps
      id: deploy
      shell: bash
      env:
        # Action defaults
        GITHUB_ACTION_PATH: ${{ github.action_path }}
        BITOPS_ENVIRONMENT: deployment
        BITOPS_FAST_FAIL: 'true'

        # Action main inputs
        BITOPS_SKIP_RUN: ${{ inputs.bitops_skip_run }}
        BITOPS_CODE_ONLY: ${{ inputs.bitops_code_only }}
        BITOPS_EXTRA_ENV_VARS: ${{ inputs.bitops_extra_env_vars }}
        BITOPS_EXTRA_ENV_VARS_FILE: ${{ inputs.bitops_extra_env_vars_file }}
        TF_STACK_DESTROY: ${{ inputs.tf_stack_destroy }}
        TF_STATE_BUCKET: ${{ inputs.tf_state_bucket }}
        TF_STATE_BUCKET_DESTROY: ${{ inputs.tf_state_bucket_destroy }}
        TF_STATE_BUCKET_PROVIDER: ${{ inputs.tf_state_bucket_provider }}
        TF_TARGETS: ${{ inputs.tf_targets }}
        ANSIBLE_SKIP: ${{ inputs.ansible_skip }}

        # Deployment repo
        GH_DEPLOYMENT_INPUT_TERRAFORM: ${{ inputs.gh_deployment_input_terraform }}
        GH_DEPLOYMENT_INPUT_ANSIBLE: ${{ inputs.gh_deployment_input_ansible }}
        GH_DEPLOYMENT_INPUT_ANSIBLE_PLAYBOOK: ${{ inputs.gh_deployment_input_ansible_playbook }}
        GH_DEPLOYMENT_INPUT_ANSIBLE_EXTRA_VARS_FILE: ${{ inputs.gh_deployment_input_ansible_extra_vars_file }}
        GH_DEPLOYMENT_ACTION_INPUT_ANSIBLE_EXTRA_VARS_FILE: ${{ inputs.gh_deployment_action_input_ansible_extra_vars_file }}
        GH_DEPLOYMENT_INPUT_HELM_CHARTS: ${{ inputs.gh_deployment_input_helm_charts }}

        # Action repo
        GH_ACTION_REPO: ${{ inputs.gh_action_repo }}
        GH_ACTION_INPUT_TERRAFORM: ${{ inputs.gh_action_input_terraform }}
        GH_ACTION_INPUT_ANSIBLE: ${{ inputs.gh_action_input_ansible }}
        GH_ACTION_INPUT_ANSIBLE_PLAYBOOK: ${{ inputs.gh_action_input_ansible_playbook }}
        GH_ACTION_INPUT_HELM_CHARTS: ${{ inputs.gh_action_input_helm_charts }}

        # AWS Specific
        AWS_ACCESS_KEY_ID: ${{ inputs.aws_access_key_id }}
        AWS_SECRET_ACCESS_KEY: ${{ inputs.aws_secret_access_key }}
        AWS_SESSION_TOKEN: ${{ inputs.aws_session_token }}
        AWS_DEFAULT_REGION: ${{ inputs.aws_default_region }}
        AWS_RESOURCE_IDENTIFIER: ${{ inputs.aws_resource_identifier }}
        AWS_ADDITIONAL_TAGS: ${{ inputs.additional_tags }}

        # ENV Files
        ENV_AWS_SECRET: ${{ inputs.env_aws_secret }}
        ENV_REPO: ${{ inputs.env_repo }}
        ENV_GHS: ${{ inputs.env_ghs }}
        ENV_GHV: ${{ inputs.env_ghv }}

        # EC2 Instance
        AWS_EC2_INSTANCE_CREATE: ${{ inputs.aws_ec2_instance_create }}
        AWS_EC2_AMI_FILTER: ${{ inputs.aws_ec2_ami_filter }}
        AWS_EC2_AMI_OWNER: ${{ inputs.aws_ec2_ami_owner }}
        AWS_EC2_AMI_ID: ${{ inputs.aws_ec2_ami_id }}
        AWS_EC2_AMI_UPDATE: ${{ inputs. aws_ec2_ami_update }}
        AWS_EC2_IAM_INSTANCE_PROFILE: ${{ inputs.aws_ec2_iam_instance_profile }}
        AWS_EC2_INSTANCE_TYPE: ${{ inputs.aws_ec2_instance_type }} 
        AWS_EC2_INSTANCE_ROOT_VOL_SIZE: ${{ inputs.aws_ec2_instance_root_vol_size }}
        AWS_EC2_INSTANCE_ROOT_VOL_PRESERVE: ${{ inputs.aws_ec2_instance_root_vol_preserve }}
        AWS_EC2_SECURITY_GROUP_NAME: ${{ inputs.aws_ec2_security_group_name }}
        AWS_EC2_CREATE_KEYPAIR_SM: ${{ inputs.aws_ec2_create_keypair_sm }}
        AWS_EC2_INSTANCE_PUBLIC_IP: ${{ inputs.aws_ec2_instance_public_ip }}
        AWS_EC2_PORT_LIST: ${{ inputs.aws_ec2_port_list }}
        AWS_EC2_USER_DATA_FILE: ${{ inputs.aws_ec2_user_data_file }}
        AWS_EC2_USER_DATA_REPLACE_ON_CHANGE: ${{ inputs.aws_ec2_user_data_replace_on_change }}

        # AWS Route53 Domains abd Certificates
        AWS_R53_ENABLE: ${{ inputs.aws_r53_enable }}
        AWS_R53_DOMAIN_NAME: ${{ inputs.aws_r53_domain_name }}
        AWS_R53_SUB_DOMAIN_NAME: ${{ inputs.aws_r53_sub_domain_name }}
        AWS_R53_ROOT_DOMAIN_DEPLOY: ${{ inputs.aws_r53_root_domain_deploy }}
        AWS_R53_ENABLE_CERT: ${{ inputs.aws_r53_enable_cert }}
        AWS_R53_CERT_ARN: ${{ inputs.aws_r53_cert_arn }}
        AWS_R53_CREATE_ROOT_CERT: ${{ inputs.aws_r53_create_root_cert }}
        AWS_R53_CREATE_SUB_CERT: ${{ inputs.aws_r53_create_sub_cert }}

        # AWS ELB
        AWS_ELB_CREATE: ${{ inputs.aws_elb_create }}
        AWS_ELB_SECURITY_GROUP_NAME: ${{ inputs.aws_elb_security_group_name }}
        AWS_ELB_APP_PORT: ${{ inputs.aws_elb_app_port }}
        AWS_ELB_APP_PROTOCOL: ${{ inputs.aws_elb_app_protocol }}
        AWS_ELB_LISTEN_PORT: ${{ inputs.aws_elb_listen_port }}
        AWS_ELB_LISTEN_PROTOCOL: ${{ inputs.aws_elb_listen_protocol }}
        AWS_ELB_HEALTHCHECK: ${{ inputs.aws_elb_healthcheck }}
                
        # AWS EFS
        AWS_EFS_CREATE: ${{ inputs.aws_efs_create }}
        AWS_EFS_CREATE_HA: ${{ inputs.aws_efs_create_ha }}
        AWS_EFS_MOUNT_ID: ${{ inputs.aws_efs_mount_id }}
        AWS_EFS_MOUNT_SECURITY_GROUP_ID: ${{ inputs.aws_efs_mount_security_group_id }}
        AWS_EFS_SECURITY_GROUP_NAME: ${{ inputs.aws_efs_security_group_name }}
        AWS_EFS_CREATE_REPLICA: ${{ inputs.aws_efs_create_replica }}
        AWS_EFS_ENABLE_BACKUP_POLICY: ${{ inputs.aws_efs_enable_backup_policy }}
        AWS_EFS_ZONE_MAPPING: ${{ inputs.aws_efs_zone_mapping }}
        AWS_EFS_TRANSITION_TO_INACTIVE: ${{ inputs.aws_efs_transition_to_inactive }}
        AWS_EFS_REPLICATION_DESTINATION: ${{ inputs.aws_efs_replication_destination }}
        AWS_EFS_MOUNT_TARGET: ${{ inputs.aws_efs_mount_target }}
        AWS_EFS_EC2_MOUNT_POINT: ${{ inputs.aws_efs_ec2_mount_point }}

        # AWS RDS
        AWS_AURORA_ENABLE: ${{ inputs.aws_aurora_enable }}
        AWS_AURORA_ENGINE:  ${{ inputs.aws_aurora_engine }}
        AWS_AURORA_ENGINE_VERSION:  ${{ inputs.aws_aurora_engine_version }}
        AWS_AURORA_DATABASE_GROUP_FAMILY:  ${{ inputs.aws_aurora_database_group_family }}
        AWS_AURORA_INSTANCE_CLASS: ${{ inputs.aws_aurora_instance_class }}
        AWS_AURORA_SECURITY_GROUP_NAME: ${{ inputs.aws_aurora_security_group_name }}
        AWS_AURORA_SUBNETS: ${{ inputs.aws_aurora_subnets }}
        AWS_AURORA_CLUSTER_NAME: ${{ inputs.aws_aurora_cluster_name }}
        AWS_AURORA_DATABASE_NAME: ${{ inputs.aws_aurora_database_name }}
        AWS_AURORA_DATABASE_PORT: ${{ inputs.aws_aurora_database_port}}
        AWS_AURORA_RESTORE_SNAPSHOT: ${{ inputs.aws_aurora_restore_snapshot }}
        AWS_AURORA_SNAPSHOT_NAME: ${{ inputs.aws_aurora_snapshot_name }}
        AWS_AURORA_SNAPSHOT_OVERWRITE: ${{ inputs.aws_aurora_snapshot_overwrite }}
        AWS_AURORA_DATABASE_PROTECTION: ${{ inputs.aws_aurora_database_protection }}
        AWS_AURORA_DATABASE_FINAL_SNAPSHOT: ${{ inputs.aws_aurora_database_final_snapshot }}

        # Docker
        DOCKER_INSTALL: ${{ inputs.docker_install }}
        DOCKER_REMOVE_ORPHANS: ${{ inputs.docker_remove_orphans }}
        DOCKER_FULL_CLEANUP: ${{ inputs.docker_full_cleanup }}
        DOCKER_REPO_APP_DIRECTORY: ${{ inputs.docker_repo_app_directory }}
        DOCKER_REPO_APP_DIRECTORY_CLEANUP: ${{ inputs.docker_repo_app_directory_cleanup }}
        DOCKER_EFS_MOUNT_TARGET: ${{ inputs.docker_efs_mount_target }}

        # AWS EKS
        AWS_EKS_CREATE: ${{ inputs.aws_eks_create }}
        AWS_EKS_REGION: ${{ inputs.aws_eks_region }}
        AWS_EKS_SECURITY_GROUP_NAME_MASTER: ${{ inputs.aws_eks_security_group_name_master }}
        AWS_EKS_SECURITY_GROUP_NAME_WORKER: ${{ inputs.aws_eks_security_group_name_worker }}
        AWS_EKS_ENVIRONMENT: ${{ inputs.aws_eks_environment }}
        AWS_EKS_STACKNAME: ${{ inputs.aws_eks_stackname }}
        AWS_EKS_CIDR_BLOCK: ${{ inputs.aws_eks_cidr_block }}
        AWS_EKS_WORKSTATION_CIDR: ${{ inputs.aws_eks_workstation_cidr }}
        AWS_EKS_AVAILABILITY_ZONES: ${{ inputs.aws_eks_availability_zones }}
        AWS_EKS_PRIVATE_SUBNETS: ${{ inputs.aws_eks_private_subnets }}
        AWS_EKS_PUBLIC_SUBNETS: ${{ inputs.aws_eks_public_subnets }}
        AWS_EKS_CLUSTER_NAME: ${{ inputs.aws_eks_cluster_name }}
        AWS_EKS_CLUSTER_LOG_TYPES: ${{ inputs.aws_eks_cluster_log_types }}
        AWS_EKS_CLUSTER_VERSION: ${{ inputs.aws_eks_cluster_version }}
        AWS_EKS_INSTANCE_TYPE: ${{ inputs.aws_eks_instance_type }}
        AWS_EKS_INSTANCE_AMI_ID: ${{ inputs.aws_eks_instance_ami_id }}
        AWS_EKS_INSTANCE_USER_DATA_FILE: ${{ inputs.aws_eks_instance_user_data_file }}
        AWS_EKS_EC2_KEY_PAIR: ${{ inputs.aws_eks_ec2_key_pair }}
        AWS_EKS_STORE_KEYPAIR_SM: ${{ inputs.aws_eks_store_keypair_sm }}
        AWS_EKS_DESIRED_CAPACITY: ${{ inputs.aws_eks_desired_capacity}}
        AWS_EKS_MAX_SIZE: ${{ inputs.aws_eks_max_size }}
        AWS_EKS_MIN_SIZE: ${{ inputs.aws_eks_min_size }}

      run: |
        echo "Running operations/_scripts/deploy/deploy.sh"
        $GITHUB_ACTION_PATH/operations/_scripts/deploy/deploy.sh
        echo "Running operations/_scripts/deploy/export_vars.sh"
        $GITHUB_ACTION_PATH/operations/_scripts/deploy/export_vars.sh
        echo "Finished executions - Now to print results"

    # always output results to GitHub Summary UI
    - name: Generate Summary Output
      if: ${{ always() }}
      shell: bash
      env: 
        SUCCESS: ${{ job.status }} # success, failure, cancelled
        URL_OUTPUT: ${{ steps.deploy.outputs.vm_url }}
        BITOPS_CODE_ONLY: ${{ inputs.bitops_code_only }}
        BITOPS_CODE_STORE: ${{ inputs.bitops_code_store }}
        TF_STACK_DESTROY: ${{ inputs.tf_stack_destroy }}
        TF_STATE_BUCKET_DESTROY: ${{ inputs.tf_state_bucket_destroy }}
      run: $GITHUB_ACTION_PATH/operations/_scripts/deploy/summary.sh

    # upload generated artifacts to GitHub if enabled
    - if: ${{ inputs.bitops_code_store == 'true' }}
      name: Archive production artifacts
      uses: actions/upload-artifact@v3
      with:
        name: Storing code - Terraform
        retention-days: 5
        path: |
          ${{ github.action_path }}/operations/deployment
          !${{ github.action_path }}/operations/deployment/**/ghs.env